---
title: "Reproducible `R` workflow integrating models in computer vision and statistical ecology: Illustration of a trade-off between deep learning for species identification and inferring spatial co‐occurrence"
author: "Olivier Gimenez, Maëlis Kervellec, Anna Chaine, Jean-Baptiste Fanjul, Lucile Marescot, Christophe Duchamp (qui d'autre? les tagueurs/tagueuses?)"
date: "`r Sys.Date()`"
always_allow_html: true
output:
  html_document:
    highlight: tango
    theme: yeti
    toc: yes
    toc_depth: 2
    number_sections: yes
link-citations: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, 
                      fig.width = 8, 
                      fig.height = 8, 
                      echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      cache = TRUE)
options(htmltools.dir.version = FALSE)
set.seed(2022) # because 2019, 2020 and 2021 were not great
library(tidyverse)
theme_set(theme_light(base_size = 14))
library(sf)
library(cowplot)
library(lubridate)
library(stringi)
library(kableExtra)
```

```{r speciesdata, include = FALSE}
# Get Jura pix
load("dat/metadata_Jura.RData")
# nrow(allfiles)
allfiles %>%
  separate(DateTimeOriginal, c("years", "months", "days", "hours", "mins", "secs")) %>%
  mutate(years = replace(years, str_detect(SourceFile, "!! ATTENTION 2017 AU LIEU DE 2016 !!"), "2017")) %>%
  mutate(years = replace(years, str_detect(years, "2006"), "2016")) %>%
  mutate(years = replace(years, str_detect(years, "2011"), "2016")) %>%
  mutate(years = replace(years, str_detect(years, "2012"), "2016")) %>%
  mutate(years = as.numeric(years), 
         months = as.numeric(months),  
         days = as.numeric(days), 
         hours = as.numeric(hours), 
         mins = as.numeric(mins), 
         secs = as.numeric(secs)) %>% 
  mutate(y = years, 
         m = months,
         d = days) %>% 
  unite(Date, years, months, days, sep="-") %>%
  mutate(h = hours, mi = mins, s = secs) %>% 
  unite(Time, hours, mins, secs, sep=":") %>%
  unite(DateTime, Date, Time, sep=" ") %>%
  mutate(DateTime = ymd_hms(DateTime)) %>%
  select(FileName, Directory, Keywords, DateTime, y, m, d, h , mi, s) -> allpic_Jura
# min(allpic_Jura$DateTime)
# max(allpic_Jura$DateTime)
# Get trap id.
trapic_final_Jura <- allpic_Jura %>% 
  separate(FileName, c("value", "value2", "key"), extra = "merge") %>% 
  unite("n_point",value, value2, sep=".") %>% 
  arrange(desc(as.numeric(n_point))) %>% 
  separate_rows(sep=",", Keywords, convert = TRUE) %>% 
  mutate(especes = Keywords) %>%
  mutate(especes = stri_replace_all_fixed(especes, "vehicule", "humain"),
         especes = stri_replace_all_fixed(especes, "chien", "humain"),
         especes = stri_replace_all_fixed(especes, "cavalier", "humain"),
         especes = stri_replace_all_fixed(especes, "chasseur", "humain"),
         especes = stri_replace_all_fixed(especes, "frequentationhumaine", "humain"),
         especes = stri_replace_all_fixed(especes, "vaches", "humain")) %>%
  filter(especes %in% c('lynx','chevreuil','chamois','chat','renard'))
# Format occupancy data
tocc <- trapic_final_Jura %>%
  mutate(especes = as_factor(especes),
         n_point = as_factor(n_point)) %>%
  mutate(mois = month(DateTime)) %>%
  group_by(mois,n_point,especes,.drop = FALSE) %>%
  count() %>%
  filter(between(mois, 3, 11)) 
# Table of detections and non-detections for lynx. 
dat_lynx <- tocc %>%
  filter(especes == 'lynx') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
# Roe deer
dat_chevreuil <- tocc %>%
  filter(especes == 'chevreuil') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
dat_chevreuil
# Chamois
dat_chamois <- tocc %>%
  filter(especes == 'chamois') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
# Fox
dat_renard <- tocc %>%
  filter(especes == 'renard') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
# Cat
dat_chat <- tocc %>%
  filter(especes == 'chat') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
# Gather everything for later use
dat_jura <- list(deer = dat_chevreuil,
                 lynx = dat_lynx,
                 chamois = dat_chamois,
                 cat = dat_chat,
                 fox = dat_renard)

# Get Ain data
load('dat/metadata_Ain.RData')
# nrow(allfiles)
allfiles %>%
  mutate(Keywords = observed) %>% # pick manual tags
  separate(DateTimeOriginal, c("years", "months", "days", "hours", "mins", "secs")) %>%
  mutate(years = as.numeric(years), 
         months = as.numeric(months),  
         days = as.numeric(days), 
         hours = as.numeric(hours), 
         mins = as.numeric(mins), 
         secs = as.numeric(secs)) %>% 
  mutate(y = years, 
         m = months,
         d = days) %>% 
  unite(Date, years, months, days, sep="-") %>%
  mutate(h = hours, mi = mins, s = secs) %>% 
  unite(Time, hours, mins, secs, sep=":") %>%
  unite(DateTime, Date, Time, sep=" ") %>%
  mutate(DateTime = ymd_hms(DateTime)) %>%
  mutate(FileName = pix) %>%
  select(FileName, Directory, Keywords, DateTime, y, m, d, h , mi, s) -> allpic_Ain
# min(allpic$DateTime, na.rm = TRUE)
# max(allpic$DateTime, na.rm = TRUE)
# Get trap id
trapic_final_Ain <- allpic_Ain %>% 
  separate(FileName, c("value", "value2", "key"), extra = "merge") %>% 
  unite("n_point",value, value2, sep=".") %>% 
  arrange(desc(as.numeric(n_point))) %>% 
  filter(Keywords %in% c('lynx','chevreuil','chamois','chat','renard')) %>%
  mutate(Keywords = fct_drop(Keywords))
# Format occupancy data
tocc <- trapic_final_Ain %>%
  mutate(especes = as_factor(Keywords),
         n_point = as_factor(n_point)) %>%
  mutate(mois = month(DateTime)) %>%
  group_by(mois,n_point,especes,.drop = FALSE) %>%
  count() %>%
  filter(between(mois, 3, 11)) 
# Table of detections and non-detections for roe deer. 
dat_chevreuil <- tocc %>%
  filter(especes == 'chevreuil') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
# lynx
dat_lynx <- tocc %>%
  filter(especes == 'lynx') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
# Chamois. 
dat_chamois <- tocc %>%
  filter(especes == 'chamois') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
# Fox
dat_renard <- tocc %>%
  filter(especes == 'renard') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
# Cat
dat_chat <- tocc %>%
  filter(especes == 'chat') %>% 
  mutate(counts = if_else(n == 0, 0, 1)) %>%
  ungroup() %>%
  select(mois,n_point,counts) %>%
  pivot_wider(names_from = mois, values_from = counts) %>% 
  as.data.frame() 
# Gather data together
dat_ain <- list(deer = dat_chevreuil,
                lynx = dat_lynx,
                chamois = dat_chamois,
                cat = dat_chat,
                fox = dat_renard)
```


<button type="button" class="btn" title="Print to PDF" onClick="window.print()">Export to PDF</button>


# Abstract

Blabla. 

# Introduction

Computer vision is a field of artificial intelligence in which a machine is taught how to extract and interpret the content of an image [@NIPS2012_4824]. Computer vision relies on deep learning that allows computational models to learn from training data -- a set of manually labelled images -- and make predictions on new data -- a set of unlabelled images [@baraniuk_science_2020; @lecun_deep_2015]. With the growing availability of massive data, computer vision with deep learning is being increasingly used to perform important tasks such as object detection, face recognition, action and activity recognition or human pose estimation in fields as diverse as medicine, robotics, transportation, genomics, sports and agriculture [@andina_deep_2018]. 

In ecology in particular, there is a growing interest in deep learning for automatizing repetitive analyses on large amount of images, such as identifying plant and animal species, distinguishing individuals of the same or different species, counting individuals or detecting relevant features [@christin_applications_2019; @lamba_deep_2019; @weinstein_computer_2018]. By saving hours of manual data analyses and tapping into massive amounts of data that keep accumulating with technological advances, deep learning has the potential to become an essential tool for ecologists and applied statisticians.

Despite the promising future of computer vision and deep learning, there are challenging issues toward their wide adoption by the community of ecologists [@wearn_responsible_2019]. First, there is a programming barrier as most, if not all, algorithms are written in the `Python` language while most ecologists are better versed in `R`  [@lai_evaluating_2019]. If ecologists are to use computer vision in routine, there is a need for bridges between these two languages (through, e.g., the `reticulate` package @reticulate_ref or the `shiny` package @tabak_improving_2020). Second, most recent applications of computer vision via deep learning in ecology **(short WoS review and Table?)** have focused on computational aspects and simple tasks without addressing the underlying ecological questions [@sutherland_identification_2013], or carrying out the statistical data analysis [@gimenez_statistical_2014]. Although perfectly understandable given the challenges at hand, we argue that a better integration of the *why* (ecological questions), the *what* (data) and the *how* (statistics) would be beneficial to computer vision for ecology [see also @weinstein_computer_2018]. **(Develop here, speak about tradeoffs, and relevance)**

Here, we showcase a full why-what-how workflow in `R` using a case study on elucidating the structure of an ecological community (a set of co-occurring species), namely that of the Eurasian lynx (*Lynx lynx*) and its main preys. First, we introduce the case study and motivate the need for deep learning. Second we illustrate deep learning for the identification of animal species in large amounts of images, including model training and validation with a dataset of labelled images, and prediction with a new dataset of unlabelled images. Last, we proceed with the quantification of spatial co-occurrence using statistical models. **(Main conclusion no need to go too far in the DL to get reasonable answer to ecological question)** We hope that our reproducible workflow will be useful to ecologists and applied statisticians. 

# Collecting images with camera traps

Lynx (*Lynx lynx*) went extinct in France at the end of the 19th century due to habitat degradation, human persecution and decrease in prey availability [@vandel_distribution_2005]. The species was reintroduced in Switzerland in the 1970s [@breitenmoser_large_1998], then re-colonised France through the Jura mountains in the 1980s [@vandel_distribution_2005]. The species is listed as endangered under the 2017 IUCN Red list and is of conservation concern in France due to habitat fragmentation, poaching and collisions with vehicles. The Jura holds the bulk of the French lynx population. 

To better understand its distribution, we need to quantify its interactions with its main preys, roe deer (*Capreolus capreolus*) and chamois (*Rupicapra rupicapra*) [@molinari-jobin_variation_2007], two ungulate species that are also hunted. To assess the relative contribution of predation and hunting, a predator-prey program was set up jointly by the French Office for Biodiversity, the Federations of Hunters from the Jura, Ain and Haute-Savoie counties and the French National Centre for Scientific Research. 

Animal detections were made using a set of camera traps in the Jura mountains that were deployed in the Jura and Ain counties (see Figure 1). We divided the two study areas into grids of 2.7 $\times$ 2.7 km cells or sites hereafter [@zimmermann_optimizing_2013] in which we set two camera traps per site (Xenon white flash with passive infrared trigger mechanisms, model Capture, Ambush and Attack; Cuddeback), with `r lapply(dat_jura, nrow)$lynx` sites in the Jura study area, and `r lapply(dat_ain, nrow)$lynx` in the Ain study area that were active over the study period (from February 2016 to October 2017 for the Jura county, and from February 2017 to May 2019 for the Ain county). Camera traps were checked weekly to change memory cards, batteries and to remove fresh snow after heavy snowfall. 

```{r studysite, fig.width = 10, fig.cap = "**Figure 1**: Study area, grid and camera trap locations."}
site_jura <- st_read("shp/SIG_39/communes_site_etude_jura.shp", quiet = TRUE)
area_jura <- st_read("shp/SIG_39/grille_jura_2.7x2.7.shp", quiet = TRUE)
trap_jura <- st_read("shp/SIG_39/Pieges_photos_JURA_l93.shp", quiet = TRUE)
st_crs(site_jura) <-  2154
st_crs(area_jura) <-  2154
st_crs(trap_jura) <-  2154

fig1a <- ggplot() + 
  geom_sf(data = site_jura) +
  geom_sf(data = area_jura,  alpha = 0.5) + 
  geom_sf(data = trap_jura, col = "black", shape = 4, size = 3) +  
  ggtitle("A. Jura county") 

site_ain <- st_read("shp/SIG_01/Ain_ZE.shp", quiet = TRUE) # l'aire d'etude
area_ain <- st_read("shp/SIG_01/Maille_2.7_Ain.shp", quiet = TRUE) # la grille pour les pieges
trap_ain <- st_read("shp/SIG_01/PP01_restreint.shp", quiet = TRUE)
st_crs(site_ain) <-  2154
st_crs(area_ain) <-  2154
st_crs(trap_ain) <-  2154

fig1b <- ggplot() + 
  geom_sf(data = site_ain) +
  geom_sf(data = area_ain,  alpha = 0.5) + 
  geom_sf(data = trap_ain, col = "black", shape = 4, size = 3) +
  ggtitle("B. Ain county")

plot_row <- plot_grid(fig1a, fig1b)
plot_row
```

```{r}
species_Jura <- allpic_Jura %>% 
  separate(FileName, c("value", "value2", "key"), extra = "merge") %>% 
  unite("n_point",value, value2, sep=".") %>% 
  arrange(desc(as.numeric(n_point))) %>% 
  separate_rows(sep=",", Keywords, convert = TRUE) %>% 
  mutate(especes = as_factor(Keywords)) %>%
  filter(!is.na(especes), 
         !especes%in%c("Non identifié", "Non id", "Non iden", "")) %>%
  mutate(especes = fct_recode(especes, 
                              "sanglier" = "sangli",
                              "humain" = "humai",
                              "chat" = "chat forestier",
                              "chat" = "chat domestique",
                              "chasseur" = "chasse",
                              "sanglier" = "sangliers",
                              "blaireau" = "blaireaux")) 
species_Ain <- allpic_Ain %>% 
  separate(FileName, c("value", "value2", "key"), extra = "merge") %>% 
  unite("n_point",value, value2, sep=".") %>% 
  arrange(desc(as.numeric(n_point))) %>% 
  separate_rows(sep=",", Keywords, convert = TRUE) %>% 
  mutate(especes = as_factor(Keywords)) %>%
  filter(!is.na(especes), 
         !especes%in%c("Non identifié", "Non id", "Non iden", "")) %>%
  mutate(especes = fct_recode(especes, 
                              "sanglier" = "sangliers",
                              "blaireau" = "blaireaux")) 
```


In total, `r nrow(species_Jura)` and `r nrow(species_Ain)` pictures were considered in the Jura and Ain sites respectively after manually droping empty pictures and pictures with unidentified species. We identified the species present on all images by hand (see Table 1) using `digiKam` a free open-source digital photo management application (<https://www.digikam.org/>). This operation took several weeks of labor full time, which is often identified as a limitation for camera trap studies. Computer vision with deep learning has been identified as a promising approach to expedite this tedious task [@norouzzadeh_deep_2021; @tabak_machine_2019; @willi_identifying_2019]. 

```{r categories, results="asis", cache = FALSE}
cat("
<style>
caption {
      color: black;
      font-size: 1.2em;
    }
</style>
")
n_Jura <- species_Jura %>% count(especes, sort = TRUE) %>% slice_head(n = 10) %>% 
  mutate(especes = fct_recode(especes,
                              "human" = "humain",
                              "vehicule" = "véhicule",
                              "dog" = "chien",
                              "fox" = "renard",
                              "wild board" = "sanglier",
                              "roe deer" = "chevreuil",
                              "cat" = "chat",
                              "badger" = "blaireau")) %>%
  rename("Species in Jura study site" = especes)
n_Ain <- species_Ain %>% count(especes, sort = TRUE) %>% slice_head(n = 10) %>% 
  mutate(especes = fct_recode(especes,
                              "human" = "humain",
                              "dog" = "chien",
                              "fox" = "renard",
                              "wild board" = "sanglier",
                              "roe deer" = "chevreuil",
                              "cat" = "chat",
                              "badger" = "blaireau",
                              "hunter" = "chasseur",
                              "rider" = "cavalier")) %>%
  rename("Species in Ain study site" = especes)
cbind(n_Jura, n_Ain) %>% kable(caption = "**Table 1**: Species identified in the Jura and Ain study sites with samples size (n). Only species with at least 100 images are shown.") %>% kable_styling()
```


# Deep learning for species identification

## General overview

I used transfer learning to fine-tune a pre-trained CNN (resnet50) using the annotated pictures from the Jura site. Then I compared the predictions from my new model for the pictures from the Ain site with the manual annotations for these pictures. Transfer learning was achieved with GPU machines. 

We use the fastai package that provides R wrappers to [fastai](https://github.com/fastai/fastai). The [fastai library](https://henry090.github.io/fastai/) simplifies training of CNNs. 
```{r}
library(fastai)
```

Expliquer principe général et les étapes ci-dessous. Below with CPU for reproducibility, subsample of picture datasets, only a few for automatic tagging. But results are proovided with GPU, more epochs and all pictures. Fully-trained model, all pictures, provided via Zenodo. 

C'est là qu'entre en jeu le deep learning, de plus en plus utilisé en écologie, voir par exemple @christin_applications_2019. L'idée est de nourrir les algorithmes avec des photos en entrée pour en sortie récupérer l'espèce qui se trouve sur la photo. Nous avons utilisé [la librairie fast-ai](https://docs.fast.ai/) qui repose sur le language Python et sa librairie Pytorch. Un avantage de cette librairie est qu'elle vient avec [un package `R` `fastai`](https://eagerai.github.io/fastai/index.html) qui propose plusieurs fonctions pour l'utiliser. 

## Model training and validation - Jura site

Quels sont les résultats obtenus? Nous avons d'abord fait du transfer learning sur un site d'étude dans le Jura où nous avions des photos déjà étiquetées. Nous avons utilisé un modèle resnet50 déjà pré-entrainé. Nous arrivons à classifier le lynx, et ses proies, le chamoix et le chevreuil, avec un degré de certitude satisfaisant. 

## Prediction - Ain site

Ensuite, nous avons utilisé le modèle pour étiqueter automatiquement des photos prises avec des pièges installés sur un autre site, dans l'Ain. Ces photos ont aussi été étiquetées à la main, on connait donc la vérité. 

# Spatial co-occurrence

Les modèles d’occupation, tels que celui de Rota et al. (2016), supposent que l’aire
d’étude est fermée, que l’état d’occupation d’un site ne change pas au cours de la période
d’étude et que les sites sont indépendants. Pour s’assurer de respecter les hypothèses d’application
du modèle, nous découpons les données de détection en trois saisons basées sur
la biologie du lynx. La première saison est la période hivernale. Elle s’étend du 1er octobre
2016 au 31 janvier 2017. A cette période les femelles sont mobiles et sont suivies de leurs
jeunes en apprentissage. La deuxième saison est la période printanière qui dure du 1er février
au 31 mai 2017. Au cours de cette saison l’activité du lynx est la plus élevée. Les
jeunes de l’année passée se séparent de leur mère et dispersent entre janvier et avril. Ils
sont alors à la recherche d’un territoire. De février à mi-avril, la période de reproduction a
lieu. Puis les femelles s’isolent pour mettre bas. Finalement lors de la troisième période (estivale),
du 1er juin au 30 septembre 2017, les femelles mettent bas après une période de 67 à
70 jours. Pendant cette période et jusqu’à 6 à 9 semaines après la naissance des jeunes, la
mère reste sédentaire. Les femelles sont, dès lors, suivies des jeunes en apprentissage
(Breitenmoser-Würsten et al. 2007a; Drouilly 2019).

Pour chaque saison, les photos brutes sont transformées en données de détection.
Plus précisément, nous construisons une matrice de détection par espèce, composée de 0
(non-détection de l’espèce) et de 1 (détection de l’espèce). Les lignes représentent les
pièges et les colonnes les occasions de capture. La capture est ici la prise d’une image par
un piège photographique. Dans notre cas, l’occasion de capture est définie comme une semaine
(Gimenez et al. 2019). Lorsque les pièges ne sont pas actifs à certaines occasions de
capture, nous spécifions que les données ne sont pas disponibles, et les semaines de capture
de ces pièges ne sont pas prises en compte dans l’analyse. Nous évitons ainsi de
confondre la non-collecte de données avec une non-détection des espèces. De plus, lorsqu’un
piège ne capture pas de lynx pendant une longue période, il est déplacé ou supprimé
et change alors de nom. Ces changements ne posent pas de problèmes lors de l’analyse,
puisque celle-ci n’est pas spatialisée et que chaque piège est supposé indépendant des
autres.

Sur la base du nombre de faux négatifs (une photo sur laquelle on a un lynx mais on prédit une autre espèce) et de faux positifs (une photo sur laquelle on n'a pas de lynx mais on prédit qu'il y en a un), les résultats sont peu satisfaisants. Toutefois, la question est de savoir si le manque de précision nuit à l'inférence des interactions prédateur-proie. Pour ce faire, on a utilisé des modèles statistiques qui permettent d'inférer les co-occurrences entre espèces en tenant compte de la difficulté de les détecter sur le terrain. Ce sont les modèles d'occupancy développés par @rota2016 et implémentés dans `R` par @fiske2011. 

On obtient les probabilités de présence du lynx, conditionnellement à la présence ou absence de ses proies (Figure 1). Il y a un léger biais dans l'estimation de la probabilité de présence du lynx sachant la présence de ses deux proies favorites quand on se fie à l'étiquetage automatique des photos. Etant donné que les différences ne sont pas énormes, l'écologue pourra décider de les ignorer au regard du temps gagné par rapport à un étiquetage à la main. Maintenant le biais est plus important sur la probabilité de présence du lynx sachant la présence du chevreuil et l'absence du chamois qui elle est sous-estimée.

![Probabilités de présence du lynx, conditionnellement à la présence ou absence de ses proies. En rouge, avec les photos étiquetées à la main. En gris-bleu, avec les photos étiquetées automatiquement.](figs/conditional_occ.jpg)

# Discussion

En conclusion, l'utilisation d'un modèle entrainé sur un site pour prédire sur un autre site est délicate. Il est facile de se perdre dans les dédales du deep learning, mais il faut garder le cap de la question écologique, et on peut accepter des performances moyennes des algorithmes si le biais engendré sur les indicateurs écologiques est faible. Malgré tout, on peut faire mieux, et nous développons actuellement des modèles de distribution d'espèce qui prendrait à la fois en compte les interactions et les faux positifs et faux négatifs. Pour aller plus loin avec le deep learning et l'analyse d'images, nous renvoyons vers @miele2021. 

+ Summarise what we did. 

+ Main lessons.

+ To be extended. 
    i) More pix, various environments, 
    ii) at once à la Yolo, ou detect and identify. 
    iii) ici pas d'images vides, mais MegaDetector sinon. Extra layer of training. 
    iv) Simulations for investifating further bias with false positives/negatives. Citer deux ou trois papiers que j'ai vus. 
    v) Ajouter des covariables. 
    vi) more interdisciplinary teams, so that mutual cross-fertilization. Algo more adapted to address ecological questions, and ecologists more able to formulate questions that would be of interest for computer scientists (voir mon thread suite aux journées du GDR)
    vii) cite imaginecology somewhere
    viii) citer papier(s) Vincent [@miele_revisiting_2021], et Jose sur Methods in Conservation dans Frontiers [@lahoz-monfort_comprehensive_2021].
    ix) We argue that maybe not worth spending valuable time tuning algo to increase perf by one or two percent, if ecological indicators not too much affected. Entirely in R with which ecologists are familiar, and entirely reproducible. 
    
Ongoing work to include covariates.

The lynx (Lynx lynx, Linné 1758) is yet again present into the Jura Mountains since the 80's.
In order to sustain his return into a functional ecosystem, we need to understand the factors
that affect lynx and his preys distribution, the roe deer (Capreolus capreolus, Linné 1758)
and the chamois (Rupicapra rupicapra, Linné 1758). How do environmental variables (forest
cover, human disturbance) affect lynx and his preys presence and their co-occurrence in the
French Jura ? What is the relative contribution of habitat preferences and prey-predator relationships
? To answer these questions, we used the multi-species occupancy model developed
by Rota et al. (2016) which accounts for species imperfect detection. Thanks to this model,
we quantified the lynx presence in function of environmental variables and the presence
and absence of his preys, by using data from a non invasive camera trapping monitoring protocol.
We show that the lynx and his preys presence is equally influenced by forest cover and
the presence of lynx, chamois or roe deer. Therefore, we need to account for interactions
between species in relation with habitat quality in inferring the occupancy of these species.
    

For much of the last
century, ecologists have typically interpreted the diversity and
composition of communities as the outcome of local-scale processes,
both biotic (e.g. competition and predation) and abiotic
(e.g. temperature and nutrients).

Some of the most challenging questions in ecology concern
communities: sets of co-occurring species.



<!-- # Appendix: Reproducible example of species identification on camera trap images with CPU -->

<!-- In this section, we go through a reproducible example of the entire deep learning workflow, from data preparation, training of a model and its validation, to the automatic labeling of pictures. We used a subsample of 467 pictures from the original dataset in the Jura county, plus 14 pictures from the original dataset in the Ain county, to allow the training of our model with CPU on a personal computer. We provide all pictures and the models fully trained with GPU from Zenodo. -->

<!-- ## Training and validation datasets -->

<!-- We first split the dataset of pictures in two datasets, a dataset for training, and the other one for validation.  -->

<!-- We use the `exifr` package to extract metadata from pictures, get a list of pictures names and extract the species from these.  -->
<!-- ```{r } -->
<!-- library(exifr) -->
<!-- pix_folder <- 'pix/pixJura/' -->
<!-- file_list <- list.files(path = pix_folder,  -->
<!--                         recursive = TRUE,  -->
<!--                         pattern = "*.JPG",  -->
<!--                         full.names = TRUE) -->
<!-- labels <-  -->
<!--   read_exif(file_list) %>% -->
<!--   as_tibble() %>% -->
<!--   unnest(Keywords, keep_empty = TRUE) %>% # keep_empty = TRUE keeps pix with no labels (empty pix) -->
<!--   group_by(SourceFile) %>% -->
<!--   slice_head() %>% # when several labels in a pix, keep first only -->
<!--   ungroup() %>% -->
<!--   mutate(Keywords = as_factor(Keywords)) %>% -->
<!--   mutate(Keywords = fct_explicit_na(Keywords, "wo_tag")) %>% # when pix has no tag -->
<!--   select(SourceFile, FileName, Keywords) %>%  -->
<!--   mutate(Keywords = fct_recode(Keywords,  -->
<!--                                # "chat" = "chat domestique",  -->
<!--                                "chat" = "chat forestier", -->
<!--                                "lievre" = "lièvre", -->
<!--                                "vehicule" = "véhicule", -->
<!--                                # "ecureuil" = "écureuil", -->
<!--                                # "laporide" = "laporidés", -->
<!--                                # "ongules" = "ongulés sauvages", -->
<!--                                # "sciuridae" = "Sciuridae", -->
<!--                                "ni" = "Non identifié")) %>%  -->
<!--   filter(!(Keywords %in% c("ni", "wo_tag"))) -->
<!-- ``` -->
<!-- ```{r echo = TRUE} -->
<!-- labels %>%  -->
<!--   count(Keywords, sort = TRUE) %>%  -->
<!-- #  print(n = Inf) %>% -->
<!--   knitr::kable(caption = "Species considered, and number of pictures with these species in them.") -->
<!-- ``` -->

<!-- Then we pick 80$\%$ of the pictures for training in each category, the rest being used for validation. -->
<!-- ```{r echo = TRUE} -->
<!-- # training dataset -->
<!-- pix_train <- labels %>%  -->
<!--   select(SourceFile, FileName, Keywords) %>% -->
<!--   group_by(Keywords) %>% -->
<!--   filter(between(row_number(), 1, floor(n()*80/100))) # 80% per category -->
<!-- # validation dataset -->
<!-- pix_valid <- labels %>%  -->
<!--   group_by(Keywords) %>% -->
<!--   filter(between(row_number(), floor(n()*80/100) + 1, n())) -->
<!-- ``` -->

<!-- Eventually, we store these pictures in two distinct directories named `train` and `valid`.  -->
<!-- ```{r} -->
<!-- # create dir train/ and copy pix there, organised by categories -->
<!-- dir.create('pix/train') # create training directory -->
<!-- for (i in levels(fct_drop(pix_train$Keywords))) dir.create(paste0('pix/train/',i)) # create dir for labels  -->
<!-- for (i in 1:nrow(pix_train)){ -->
<!-- 	file.copy(as.character(pix_train$SourceFile[i]),  -->
<!-- 	          paste0('pix/train/', as.character(pix_train$Keywords[i]))) # copy pix in corresp dir -->
<!-- } -->
<!-- # create dir valid/ and copy pix there, organised by categories. -->
<!-- dir.create('pix/valid') # create validation dir -->
<!-- for (i in levels(fct_drop(pix_train$Keywords))) dir.create(paste0('pix/valid/',i)) # create dir for labels  -->
<!-- for (i in 1:nrow(pix_valid)){ -->
<!-- 	file.copy(as.character(pix_valid$SourceFile[i]),  -->
<!-- 	          paste0('pix/valid/', as.character(pix_valid$Keywords[i]))) # copy pix in corresp dir -->
<!-- } -->
<!-- # delete pictures in valid/ directory for which we did not train the model -->
<!-- to_be_deleted <- setdiff(levels(fct_drop(pix_valid$Keywords)), levels(fct_drop(pix_train$Keywords))) -->
<!-- if (!is_empty(to_be_deleted)) { -->
<!--   for (i in 1:length(to_be_deleted)){ -->
<!--     unlink(paste0('pix/valid/', to_be_deleted[i])) -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- What is the sample size of these two datasets? -->
<!-- ```{r} -->
<!-- bind_rows("training" = pix_train, "validation" = pix_valid, .id = "dataset") %>% -->
<!--   group_by(dataset) %>% -->
<!--   count(Keywords) %>%  -->
<!--   rename(category = Keywords) %>% -->
<!--   knitr::kable(caption = "Sample size (n) for the training and validation datasets.") -->
<!-- ``` -->

<!-- ## Transfer learning -->

<!-- We proceed with transfer learning using the pictures from the Jura county (or a subsample more exactly).  -->

<!-- We first load pictures, apply standard transformations to improve training (flip, rotate, zoom, rotate, ligth transform).  -->
<!-- ```{r} -->
<!-- dls <- ImageDataLoaders_from_folder( -->
<!--   path = "pix/", -->
<!--   train = "train", -->
<!--   valid = "valid", -->
<!--   item_tfms = Resize(size = 460),  -->
<!--   bs = 10, -->
<!--   batch_tfms = list(aug_transforms(size = 224,  -->
<!--                                    min_scale = 0.75), # transformation -->
<!--                     Normalize_from_stats( imagenet_stats() )), -->
<!--   num_workers = 0, -->
<!--   ImageFile.LOAD_TRUNCATED_IMAGES = TRUE) -->
<!-- ``` -->

<!-- Then we get the model architecture. For the sake of illustration, we use a resnet18 here, but we used a resnet50 to get the full results presented below. -->
<!-- ```{r} -->
<!-- learn <- cnn_learner(dls = dls,  -->
<!--                      arch = resnet18(),  -->
<!--                      metrics = list(accuracy, error_rate)) -->
<!-- ``` -->

<!-- Now we are ready to train our model. Again, for the sake of illustration, we use only 2 epochs here, but used 20 epochs to get the full results presented below. With all pictures and a resnet50, it took 75 minutes per epoch approximatively on a Mac with a 2.4Ghz Inter Core i9 8 cores processor and 64Go emory with CPU, and less than half an hour with GPU. On this reduced dataset, it took a bit more than a minute per epoch with CPU. Note that we save the model after each epoch for later use. -->
<!-- ```{r} -->
<!-- one_cycle <- learn %>%  -->
<!--   fit_one_cycle(2, cbs = SaveModelCallback(every_epoch = TRUE,  -->
<!--                                            fname = 'model')) -->
<!-- one_cycle -->
<!-- ``` -->

<!-- We may dig a bit deeper in training performances by loading the best model, here `model_1.pth`, and display some metrics for each species.   -->
<!-- ```{r} -->
<!-- # see model saved at each epoch -->
<!-- # list.files('pix/pix/models/') -->
<!-- learn$load("model_1") -->
<!-- interp <- ClassificationInterpretation_from_learner(learn) -->
<!-- interp$print_classification_report() -->
<!-- ``` -->

<!-- Four metrics are given.  -->

<!-- Precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. It quantifies the ability of the classifier not to label as positive a sample that is negative. The question that this metric answers is of all pix that labeled as positive, how many actually were positive? -->

<!-- The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The question recall answers is: of all the pix that were positive, how many did we label?  -->

<!-- The f1-score can be interpreted as a weighted average of the precision and recall, it reaches its best value at 1 and worst score at 0. -->

<!-- The f1-score weights recall more than precision by a factor of 1, meaning that recall and precision are equally important. f1-score is 2 * (recall * precision) / (recall + precision) -->

<!-- The support is the number of occurrences of each class in y_true (ground truth or correct target values). -->

<!-- macro avg: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. -->

<!-- weighted avg: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance. -->

<!-- Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations, it is (tp + tn)/(tp + tn + fp + fn)  -->

<!-- We may extract the categories that get the most confused. -->
<!-- ```{r} -->
<!-- interp %>% most_confused() -->
<!-- ``` -->

<!-- ## Prediction -->

<!-- In this section, we show how to use our freshly trained model to label pictures that were taken in another study site in the Ain county.  -->

<!-- First, we get the path to the pictures. -->
<!-- ```{r} -->
<!-- fls <- list.files(path = "pix/pixAin", -->
<!--                   full.names = TRUE, -->
<!--                   recursive = TRUE) -->
<!-- ``` -->

<!-- Then carry out the prediction, and compare to the truth. -->
<!-- ```{r} -->
<!-- predicted <- character(3) -->
<!-- categories <- interp$vocab %>% -->
<!--   str_replace_all("[[:punct:]]", " ") %>% -->
<!--   str_trim() %>% -->
<!--   str_split("   ") %>% -->
<!--   unlist() -->
<!-- for (i in 1:length(fls)){ -->
<!--   result <- learn %>% predict(fls[i]) # make prediction -->
<!--   result[[3]] %>%  -->
<!--     stringr::str_extract("\\d+") %>% -->
<!--     as.integer() -> index # extract relevant info -->
<!--   predicted[i] <- categories[index + 1] # match it with categories -->
<!-- } -->
<!-- data.frame(truth = c("lynx", "roe deer", "wild boar"), -->
<!--            prediction = predicted) %>% -->
<!--   knitr::kable(caption = "Comparison of the predictions vs. ground truth.") -->
<!-- ``` -->

# Session information

```{r session-info}
sessionInfo()
```

# Acknowledgments

ANR. Folks who have labeled pix if not co-authors. MBB folks. Vincent Miele for his help along the way, and being an inspiration. 

# References
